{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already setup: conda env using python3.12.9\n",
    "#and \"pip install\" done for all needed packages - also see pip freeze at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V04BpE6y4pXS"
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJWe0l2Pe-0q",
    "outputId": "0183802e-d69f-4339-c00b-1abb35d6addf"
   },
   "outputs": [],
   "source": [
    "# Vision Transformer and GPT fine-tuned for an image captioning system\n",
    "# with this kaggle dataset: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
    "\n",
    "from transformers import ViTModel\n",
    "from transformers import AutoFeatureExtractor #which converts images into tensors\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel, GPT2TokenizerFast, AutoFeatureExtractor, \\\n",
    "                         AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from PIL import Image #pip install Pillow\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor, Resize\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "#img = Image.open('./kobe.jpeg')\n",
    "img = Image.open('.\\kobe.jpeg')\n",
    "display(img)\n",
    "print(feature_extractor(img).pixel_values[0].shape)\n",
    "# (3,224,224) # 3 due to color, 224 due to feature_extractor resized the image\n",
    "\n",
    "plt.imshow(feature_extractor(img).pixel_values[0].transpose(1, 2, 0))\n",
    "# original image got (3,224,224)\n",
    "# .transpose here put the 1st dimension for colors as the last dimension\n",
    "# .. which was the 0 in (1, 2, 0), as it is 0-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thopD4zr3czZ",
    "outputId": "d405c25d-e004-4ddf-ab41-9ec75fdee93f"
   },
   "outputs": [],
   "source": [
    "# Load up a pretrained Vision Transformer\n",
    "# Many weights are innitialized randomly, namely the cross attention weights\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k', #224 for image size, in for ImageNet, 21k dataset\n",
    "    'distilgpt2' #use better/newer gemma3:1b or gemma3:4b instead? #https://huggingface.co/docs/transformers/en/model_doc/gemma3\n",
    "    # .. NO .. because this small distilgpt2 already took hours to train below\n",
    ")\n",
    "#needed this for above (per https://pytorch.org/get-started/locally/):\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "#from practical point of view nowadays, this (and others) will be much better for the task here: https://ollama.com/x/llama3.2-vision\n",
    "\n",
    "print(type(model.encoder))\n",
    "print(type(model.decoder))\n",
    "\n",
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    #total_params += numel(param)\n",
    "    total_params += param.numel()\n",
    "print(f\"Our model has a combined {total_params:,} parameters\")\n",
    "\n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUSiHsar4TI0",
    "outputId": "fde83be6-5db8-4200-a02b-08223533e901"
   },
   "outputs": [],
   "source": [
    "model # note \"patch embeddings\" below, vs text-based transformer's word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_171bMZ1ASh"
   },
   "outputs": [],
   "source": [
    "#IMAGES_PATH = './flicker_images'\n",
    "IMAGES_PATH = '.\\\\flicker_images' #for windows\n",
    "MIN_CAPTION, MAX_CAPTION = 10, 50\n",
    "\n",
    "def load_captions_data(filename):\n",
    "    with open(filename) as caption_file:\n",
    "        caption_data = caption_file.readlines()\n",
    "        caption_mapping = {}\n",
    "        text_data = []\n",
    "\n",
    "        for line in caption_data:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            # Image name and captions are separated using a tab\n",
    "            img_name, caption = line.split(\"\\t\")\n",
    "\n",
    "            # Each image is repeated five times for the five different captions.\n",
    "            # Each image name has a suffix `#(caption_number)`\n",
    "            img_name = img_name.split(\"#\")[0]\n",
    "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
    "\n",
    "            if img_name.endswith(\"jpg\"):\n",
    "                caption = caption.replace(' .', '').strip()\n",
    "                tokens = caption.strip().split()\n",
    "                if len(caption) < MIN_CAPTION or len(caption) > MAX_CAPTION:\n",
    "                    continue\n",
    "                text_data.append(caption)\n",
    "\n",
    "                if img_name in caption_mapping:\n",
    "                    caption_mapping[img_name].append(caption)\n",
    "                else:\n",
    "                    caption_mapping[img_name] = [caption]\n",
    "\n",
    "        return caption_mapping, text_data\n",
    "\n",
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data(\"./Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9Ks-LMH5j2U",
    "outputId": "c13ee20e-8dbd-47e8-f76a-0944d1d21252"
   },
   "outputs": [],
   "source": [
    "list(captions_mapping.items())[:3] #multiple captions for each image, all are considered correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75h319GQ8H4o"
   },
   "outputs": [],
   "source": [
    "normalize = Normalize(\n",
    "    mean=feature_extractor.image_mean,\n",
    "    std=feature_extractor.image_std\n",
    ")\n",
    "\n",
    "_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size=(feature_extractor.size['height'], feature_extractor.size['width'])), # Data augmentation. Randomly crop the image, then resize to 224x224\n",
    "        ToTensor(),                                  # Convert to pytorch tensor\n",
    "        normalize                                    # normalize pixel values to look like images during pre-training\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYGTsabz8NPD"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# using multiple captions per image, given data augmentation being used\n",
    "for path, captions in captions_mapping.items():\n",
    "    for caption in captions:\n",
    "        rows.append({'path': path, 'caption': caption})\n",
    "\n",
    "image_df = pd.DataFrame(rows)\n",
    "\n",
    "image_dataset = Dataset.from_pandas(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWckIEWE8Hq6",
    "outputId": "d3ec1a77-ed08-4d21-84d3-d29782adf477"
   },
   "outputs": [],
   "source": [
    "print(image_df.shape)\n",
    "print(image_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2XUFMD--u7a",
    "outputId": "05d5098d-4b28-4a02-ffbd-09de087423e0"
   },
   "outputs": [],
   "source": [
    "# this is just for debug purpose\n",
    "current_directory = os.getcwd()\n",
    "#print(\"current directory is:\", current_directory)\n",
    "entries = os.listdir(current_directory)\n",
    "\n",
    "for entry in entries:\n",
    "    print(entry)\n",
    "\n",
    "entries = os.listdir(\"./flicker_images\")\n",
    "\n",
    "for entry in entries[:10]:\n",
    "    print(entry)\n",
    "\n",
    "if \"1000268201_693b08cb0e.jpg\" in entries:\n",
    "    print(\"yes, 1000268201_693b08cb0e.jpg, which will be used later for both fine-tuned and non-fine-tuned models, is in the folder\")\n",
    "else:\n",
    "    print(\"no, 1000268201_693b08cb0e.jpg, which will be used later for both fine-tuned and non-fine-tuned models, is NOT in the folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debugging purpose\n",
    "'''\n",
    "print(image_dataset[0]) # image_dataset[0] is a dictionary with keys: 'path' and 'caption'\n",
    "print(image_dataset[0]['path']) # image_dataset[0] is a dictionary with keys: 'path' and 'caption'\n",
    "\n",
    "file_path = image_dataset[0]['path']\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Check if the file is readable\n",
    "    if os.access(file_path, os.R_OK):\n",
    "        print(f\"The file '{file_path}' exists and is readable.\")\n",
    "        # You can now safely open and read the file\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "            #with open(file_path, 'r') as file:\n",
    "                # Perform operations on the file (e.g., read its contents)\n",
    "                contents = file.read()\n",
    "                print(\"File contents:\")\n",
    "                print(contents)\n",
    "        except IOError as e:\n",
    "            print(f\"An error occurred while reading the file: {e}\")\n",
    "    else:\n",
    "        print(f\"The file '{file_path}' exists but is not readable.\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' does not exist.\")\n",
    "\n",
    "tmp_imgimg = Image.open(image_dataset[0]['path'])\n",
    "display(tmp_img)\n",
    "\n",
    "normalize = Normalize(\n",
    "    mean=feature_extractor.image_mean,\n",
    "    std=feature_extractor.image_std\n",
    ")\n",
    "print(feature_extractor.image_mean)\n",
    "_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size=(feature_extractor.size['height'], feature_extractor.size['width'])), # Data augmentation. Randomly crop the image, then resize to 224x224\n",
    "        ToTensor(),                                  # Convert to pytorch tensor\n",
    "        normalize                                    # normalize pixel values to look like images during pre-training\n",
    "    ]\n",
    ")\n",
    "_transforms(Image.open(file_path))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "79cf834b98f24bf789acfc00f25310af",
      "60b173dd5b56458fb7e6b768d79551a0",
      "ece201b4f1a14c7d8f09563bad3bb037",
      "10fd5eccf62b495c97eff94d8ac65606",
      "741ebb8a2b5648c19a507ac9d12c5f2e",
      "f82d1b4c69a641b595222a09c22bf3d8",
      "d800475a20c54537a453a440284e1be5",
      "a6af18a9f1564534b1f23dc8556d071c",
      "d7110dcd7a8a4deb819088afbc2190bc",
      "872874b3ebbc4a3ab0237105ec5c1d39",
      "a4905024e45e4620953a0ab8099aedd8"
     ]
    },
    "id": "iRueMh6p8POX",
    "outputId": "f5e3e706-6209-49e4-83b9-a1a0717c42c1"
   },
   "outputs": [],
   "source": [
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "def image_preprocess(examples):\n",
    "    # ViT expects pixel_values instead of input_ids\n",
    "    examples['pixel_values'] = [_transforms(Image.open(path)) for path in examples['path']]\n",
    "    # We are padding tokens here instead of using a datacollator\n",
    "    tokenized = gpt2_tokenizer(\n",
    "        examples['caption'], padding='max_length', max_length=10, truncation=True\n",
    "    )['input_ids']\n",
    "    # the output captions\n",
    "    examples['labels'] = [[l if l != gpt2_tokenizer.pad_token_id else -100 for l in t] for t in tokenized]\n",
    "    #setting to -100 for the pad tokens so the \"loss\" will NOT be calculated for such, as such is NOT part of the caption\n",
    "\n",
    "    # delete unused keys\n",
    "    del examples['path']\n",
    "    del examples['caption']\n",
    "    return examples\n",
    "\n",
    "image_dataset = image_dataset.map(image_preprocess, batched=True)\n",
    "\n",
    "image_dataset = image_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZGEGSSx8PGX"
   },
   "outputs": [],
   "source": [
    "# We set a pad token and a start token in our combined model to be the same as gpt2\n",
    "\n",
    "model.config.pad_token = gpt2_tokenizer.pad_token\n",
    "model.config.pad_token_id = gpt2_tokenizer.pad_token_id\n",
    "\n",
    "model.config.decoder_start_token = gpt2_tokenizer.bos_token\n",
    "model.config.decoder_start_token_id = gpt2_tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show accelerate\n",
    "!pip show transformers\n",
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cu_VteKY8O-D"
   },
   "outputs": [],
   "source": [
    "# freeze all but the last two layers in the ViT\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    if 'encoder.layer.10' in name: #10 and 11 are the last 2 layers in ViT (0 based index, 12 layers in total)\n",
    "        break\n",
    "    param.requires_grad = False\n",
    "#note that we are NOT freezing anything in the gpt2 model as the cross attention weights are all over the place in gpt2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./v1_image_captioning', # The output directory\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=4, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    log_level='info',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=image_dataset['train'],\n",
    "    eval_dataset=image_dataset['test'],\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra137DJR-E5p"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF1Hdb31-Exh"
   },
   "outputs": [],
   "source": [
    "# the loss decline is starting to slow down. This is a good indication that we may want to try training on more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jj_YTkr-Eql"
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "_CPdW2Tt-EkO",
    "outputId": "1fbf6688-2735-4bb7-a9db-3987dc7fa082"
   },
   "outputs": [],
   "source": [
    "# loading model and config from pretrained folder\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained('./v1_image_captioning')\n",
    "\n",
    "# Create a new composition that doesn't crop images for inference to make it easier for the model\n",
    "inference_transforms = Compose(\n",
    "    [\n",
    "        Resize(size=(feature_extractor.size['height'], feature_extractor.size['width'])),\n",
    "        ToTensor(),\n",
    "        normalize\n",
    "    ]\n",
    ")\n",
    "\n",
    "# a helper function to caption images from the web or a file path\n",
    "def caption_image(m, path):\n",
    "    if 'http' in path:\n",
    "        response = requests.get(path)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        image_matrix = inference_transforms(img).unsqueeze(0) #.unsqueeze(0) to add a \"batch\" dimension in the beginning\n",
    "    else:\n",
    "        img = Image.open(path)\n",
    "        image_matrix = inference_transforms(img).unsqueeze(0)\n",
    "\n",
    "    generated = m.generate(\n",
    "        image_matrix,\n",
    "        num_beams=5, #default is 1, setting to 5 is asking GPT to think ahead more on what words could be used in the future\n",
    "        max_length=20,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=5, #had this: ValueError: `num_return_sequences` (5) has to be smaller or equal to `num_beams` (3).\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    caption_options = [gpt2_tokenizer.decode(g, skip_special_tokens=True).strip() for g in generated]\n",
    "    display(img)\n",
    "    return caption_options, generated, image_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions, generated, image_matrix = caption_image(finetuned_model, './kobe.jpeg')\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Og8v7QJ-Ec5"
   },
   "outputs": [],
   "source": [
    "non_finetuned = VisionEncoderDecoderModel.from_encoder_decoder_pretrained('google/vit-base-patch16-224-in21k', 'distilgpt2')\n",
    "\n",
    "captions, generated, image_matrix = caption_image(non_finetuned, './kobe.jpeg')\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtyGMdUcCJEj"
   },
   "outputs": [],
   "source": [
    "captions, generated, image_matrix = caption_image(\n",
    "    finetuned_model, './flicker_images/1000268201_693b08cb0e.jpg'\n",
    ")\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeKTChr5Cb6v"
   },
   "outputs": [],
   "source": [
    "captions, generated, image_matrix = caption_image(\n",
    "    non_finetuned, './flicker_images/1000268201_693b08cb0e.jpg'\n",
    ")\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vit_gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10fd5eccf62b495c97eff94d8ac65606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_872874b3ebbc4a3ab0237105ec5c1d39",
      "placeholder": "​",
      "style": "IPY_MODEL_a4905024e45e4620953a0ab8099aedd8",
      "value": " 0/19599 [00:00&lt;?, ? examples/s]"
     }
    },
    "60b173dd5b56458fb7e6b768d79551a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f82d1b4c69a641b595222a09c22bf3d8",
      "placeholder": "​",
      "style": "IPY_MODEL_d800475a20c54537a453a440284e1be5",
      "value": "Map:   0%"
     }
    },
    "741ebb8a2b5648c19a507ac9d12c5f2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cf834b98f24bf789acfc00f25310af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60b173dd5b56458fb7e6b768d79551a0",
       "IPY_MODEL_ece201b4f1a14c7d8f09563bad3bb037",
       "IPY_MODEL_10fd5eccf62b495c97eff94d8ac65606"
      ],
      "layout": "IPY_MODEL_741ebb8a2b5648c19a507ac9d12c5f2e"
     }
    },
    "872874b3ebbc4a3ab0237105ec5c1d39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4905024e45e4620953a0ab8099aedd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6af18a9f1564534b1f23dc8556d071c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7110dcd7a8a4deb819088afbc2190bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d800475a20c54537a453a440284e1be5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ece201b4f1a14c7d8f09563bad3bb037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6af18a9f1564534b1f23dc8556d071c",
      "max": 19599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7110dcd7a8a4deb819088afbc2190bc",
      "value": 0
     }
    },
    "f82d1b4c69a641b595222a09c22bf3d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
