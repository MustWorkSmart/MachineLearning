{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oL0fn7Up5uyx"
      },
      "source": [
        "# spaCy\n",
        "# 1) processing pipeline - tokenization, stop words removal, POS tagging, depencece parsing, Lemmatization. NER\n",
        "# 2) processing pipeline with a file of sentiment examples/texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.11-cp312-cp312-macosx_10_13_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.15-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.13-cp312-cp312-macosx_10_13_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.12-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.10-cp312-cp312-macosx_10_13_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
            "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
            "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: wrapt in /opt/miniconda3/envs/my-env/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.2)\n",
            "Downloading spacy-3.8.11-cp312-cp312-macosx_10_13_x86_64.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.13-cp312-cp312-macosx_10_13_x86_64.whl (43 kB)\n",
            "Downloading murmurhash-1.0.15-cp312-cp312-macosx_10_13_x86_64.whl (27 kB)\n",
            "Downloading preshed-3.0.12-cp312-cp312-macosx_10_13_x86_64.whl (129 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.2-cp312-cp312-macosx_10_13_x86_64.whl (656 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.2/656.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.10-cp312-cp312-macosx_10_13_x86_64.whl (794 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.5/794.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
            "Downloading blis-1.3.3-cp312-cp312-macosx_10_13_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "Installing collected packages: wasabi, typer-slim, spacy-loggers, spacy-legacy, smart-open, murmurhash, cymem, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
            "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "#Download the English langugage model for spaCy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYVceT0cBAK1",
        "outputId": "14c40dc8-8c88-416e-9407-4fa87ef01563"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "#Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\") #spacy.load - loads the English model and assigns it to variable nlp\n",
        "\n",
        "#When you execute nlp = spacy.load('en'), spaCy downloads and loads the pre-trained English language model\n",
        "#into memory and assigns it to the variable nlp.\n",
        "#This pre-trained model contains information about word vectors, part-of-speech tags, syntactic dependencies, and other linguistic features necessary for various NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) processing pipeline - tokenization, stop words removal, POS tagging, depencece parsing, Lemmatization. NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CynCZB2eSpk4"
      },
      "source": [
        "##spaCy Procesing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3lAMG68F3_F"
      },
      "source": [
        "In spaCy, the order of tasks in the processing pipeline generally follows a predefined sequence, although it's also customizable. By default, spaCy's processing pipeline includes the following components in the specified order:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OZqsZCjD_Sh"
      },
      "source": [
        "### Order of Tasks in the Processing Pipeline\n",
        "\n",
        "| Order | Name         | Definition |\n",
        "| :-----| :------ |: ---- |\n",
        "| 1     | Tokenization | Input text is split into individual tokens, such as words and punctuation marks. |\n",
        "| 2     | Stop Words    | Removes stop words from the text. |\n",
        "| 3     | POS Tagging   | Assigns grammatical labels (e.g., noun, verb, adjective) to each token in the text based on its syntactic role within the sentence. |\n",
        "| 4     | Dependency Parsing| Analyzes the grammatical structure of the text by determining the relationships |\n",
        "| 5     |Lemmatization | Reduces tokens to their base or root form (lemmas)  |\n",
        "| 6     |  Named Entity Recognition|  Identifies, categorizes persons, organizations, locations, dates, etc..|\n",
        "| 7     | Other Use Case Tasks| May be included in pipelne (Sentiment Analysis) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55OLJ3mfrAwH",
        "outputId": "4cd10a4c-f19b-49e9-9b28-38b2417a1af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization:\n",
            "A\n",
            "customer\n",
            "in\n",
            "New\n",
            "York\n",
            "City\n",
            "wants\n",
            "to\n",
            "give\n",
            "a\n",
            "review\n",
            ".\n",
            "\n",
            "\n",
            "Filtered Tokens (without stop words):\n",
            "['customer', 'New', 'York', 'City', 'wants', 'review', '.']\n",
            "\n",
            "\n",
            "Part-of-Speech(POS) Tagging:\n",
            "A DET\n",
            "customer NOUN\n",
            "in ADP\n",
            "New PROPN\n",
            "York PROPN\n",
            "City PROPN\n",
            "wants VERB\n",
            "to PART\n",
            "give VERB\n",
            "a DET\n",
            "review NOUN\n",
            ". PUNCT\n",
            "\n",
            "\n",
            "Named Entity Recognition (NER):\n",
            "New York City GPE\n",
            "\n",
            "\n",
            "Lemmatization:\n",
            "['a', 'customer', 'in', 'New', 'York', 'City', 'want', 'to', 'give', 'a', 'review']\n"
          ]
        }
      ],
      "source": [
        "# Example text\n",
        "text = \"A customer in New York City wants to give a review.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# The nlp object is typically a loaded spaCy language model,\n",
        "# such as the English language model ('en') you loaded. When you pass\n",
        "# the text string to the nlp object, spaCy processes the text\n",
        "# through its NLP pipeline.\n",
        "\n",
        "# After executing doc = nlp(text), the variable doc contains a\n",
        "# spaCy Doc object, which represents the processed version of the input text.\n",
        "\n",
        "# Example text\n",
        "text = \"A customer in New York City wants to give a review.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text) #Recall that the Doc object represents the our sentence.\n",
        "\n",
        "# Tokenization\n",
        "print(\"Tokenization:\")\n",
        "for token in doc:\n",
        "    # Iterate through each token in the processed text and print the token text\n",
        "    print(token.text)\n",
        "\n",
        "print(\"\\n\")  # Add a newline for separation/readability\n",
        "\n",
        "# Filter out stop words\n",
        "print(\"Filtered Tokens (without stop words):\")\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "# Create a list of tokens excluding stop words using list comprehension\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\n\")  # Add a newline for separation/readability\n",
        "\n",
        "# Part-of-Speech(POS) Tagging\n",
        "print(\"Part-of-Speech(POS) Tagging:\")\n",
        "for token in doc:\n",
        "    # Iterate through each token and print the token text and its POS tag\n",
        "    print(token.text, token.pos_)\n",
        "\n",
        "print(\"\\n\")  # Add a newline for separation/readability\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"Named Entity Recognition (NER):\")\n",
        "for ent in doc.ents:\n",
        "    # Iterate through each named entity in the processed text and print its text and label\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "print(\"\\n\")  # Add a newline for separation/readability\n",
        "\n",
        "# Lemmatization\n",
        "print(\"Lemmatization:\")\n",
        "lemmatized_tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
        "# Create a list of lemmatized tokens excluding punctuation using list comprehension\n",
        "print(lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) processing pipeline with a file, for sentiment examples/texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UBSMP0lLrXRi"
      },
      "outputs": [],
      "source": [
        "file_path = './sentiment_examples.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    sentiment_texts = file.readlines()\n",
        "\n",
        "# After this block of code executes, the 'sentiment_texts' list contains\n",
        "# all the lines of text read from the 'sentiment_examples.txt' file.\n",
        "\n",
        "# The 'with' statement is used to open the file in a way that ensures it is automatically closed after the block of code inside the 'with' statement is executed.\n",
        "\n",
        "# 'open(file_path, 'r', encoding='utf-8')' opens the file in read mode ('r') with UTF-8 encoding ('utf-8').\n",
        "# 'as file' assigns the opened file object to the variable 'file', which can be used to read from the file.\n",
        "\n",
        "# 'file.readlines()' reads all lines from the file and returns a list of strings, where each string corresponds to a line of text in the file.\n",
        "# These strings are stored in the 'sentiment_texts' list for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzCjJnodmL_s",
        "outputId": "f75c1516-44e4-4d15-fc4c-91aeac1b0d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                   Sentiment Example  \\\n",
            "0  \"I love the new features of your product! It h...   \n",
            "1  \"The customer support was exceptional in New Y...   \n",
            "2  \"The quality of your service exceeded my expec...   \n",
            "3  \"I'm extremely satisfied with my purchase. The...   \n",
            "4  \"The user interface is intuitive and easy to n...   \n",
            "5  \"I had a positive experience shopping on your ...   \n",
            "6  \"Your company values customer feedback, and it...   \n",
            "7  \"The pricing is fair, and the value I get in r...   \n",
            "8  \"I appreciate the personalized recommendations...   \n",
            "9  \"The delivery was prompt, and the packaging wa...   \n",
            "\n",
            "                                              Tokens  \\\n",
            "0  [\", I, love, the, new, features, of, your, pro...   \n",
            "1  [\", The, customer, support, was, exceptional, ...   \n",
            "2  [\", The, quality, of, your, service, exceeded,...   \n",
            "3  [\", I, 'm, extremely, satisfied, with, my, pur...   \n",
            "4  [\", The, user, interface, is, intuitive, and, ...   \n",
            "5  [\", I, had, a, positive, experience, shopping,...   \n",
            "6  [\", Your, company, values, customer, feedback,...   \n",
            "7  [\", The, pricing, is, fair, ,, and, the, value...   \n",
            "8  [\", I, appreciate, the, personalized, recommen...   \n",
            "9  [\", The, delivery, was, prompt, ,, and, the, p...   \n",
            "\n",
            "                                     Filtered Tokens  \\\n",
            "0  [\", love, new, features, product, !, greatly, ...   \n",
            "1  [\", customer, support, exceptional, New, York,...   \n",
            "2  [\", quality, service, exceeded, expectations, ...   \n",
            "3  [\", extremely, satisfied, purchase, ., product...   \n",
            "4  [\", user, interface, intuitive, easy, navigate...   \n",
            "5  [\", positive, experience, shopping, website, ....   \n",
            "6  [\", company, values, customer, feedback, ,, sh...   \n",
            "7  [\", pricing, fair, ,, value, return, fantastic...   \n",
            "8  [\", appreciate, personalized, recommendations,...   \n",
            "9  [\", delivery, prompt, ,, packaging, secure, .,...   \n",
            "\n",
            "                                            POS Tags     Named Entities  \n",
            "0  [(\", PUNCT), (I, PRON), (love, VERB), (the, DE...                 []  \n",
            "1  [(\", PUNCT), (The, DET), (customer, NOUN), (su...  [(New York, GPE)]  \n",
            "2  [(\", PUNCT), (The, DET), (quality, NOUN), (of,...    [(Prague, GPE)]  \n",
            "3  [(\", PUNCT), (I, PRON), ('m, AUX), (extremely,...                 []  \n",
            "4  [(\", PUNCT), (The, DET), (user, NOUN), (interf...                 []  \n",
            "5  [(\", PUNCT), (I, PRON), (had, VERB), (a, DET),...                 []  \n",
            "6  [(\", PUNCT), (Your, PRON), (company, NOUN), (v...                 []  \n",
            "7  [(\", PUNCT), (The, DET), (pricing, NOUN), (is,...                 []  \n",
            "8  [(\", PUNCT), (I, PRON), (appreciate, VERB), (t...                 []  \n",
            "9  [(\", PUNCT), (The, DET), (delivery, NOUN), (wa...                 []  \n"
          ]
        }
      ],
      "source": [
        "# Initialize empty lists to store the results\n",
        "token_lists = []  # List to store tokens for each sentiment example\n",
        "filtered_token_lists = []  # List to store filtered tokens (after stop word removal) for each sentiment example\n",
        "pos_tag_lists = []  # List to store POS tags for each sentiment example\n",
        "ner_lists = []  # List to store named entities for each sentiment example\n",
        "\n",
        "# Process each sentiment example using spaCy and store the results\n",
        "for sentiment_text in sentiment_texts:\n",
        "    doc = nlp(sentiment_text.strip())  # Strip any leading/trailing whitespace\n",
        "    #The .strip() method is used to clean up the sentiment_text\n",
        "    #before passing it to the spaCy nlp pipeline for processing.\n",
        "    #This ensures that there are no unwanted spaces or newline characters\n",
        "    #that could affect the processing of the text by spaCy.\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = [token.text for token in doc]  # Extract tokens from the processed text\n",
        "    token_lists.append(tokens)  # Append tokens list to token_lists\n",
        "    #Creates a list of tokens and appending it to the token_lists list.\n",
        "    #This is done to store the tokens for each sentiment example.\n",
        "\n",
        "    # Stop Word Removal filter\n",
        "    filtered_tokens = [token.text for token in doc if not token.is_stop]  # Filter out stop words\n",
        "    filtered_token_lists.append(filtered_tokens)  # Append filtered tokens list to filtered_token_lists\n",
        "\n",
        "    # Part-of-Speech Tagging (POS tagging)\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]  # Extract token text and POS tags\n",
        "    pos_tag_lists.append(pos_tags)  # Append POS tags list to pos_tag_lists\n",
        "\n",
        "    # Named Entity Recognition (NER)\n",
        "    ner_entities = [(ent.text, ent.label_) for ent in doc.ents]  # Extract named entities and their labels\n",
        "    ner_lists.append(ner_entities)  # Append named entities list to ner_lists\n",
        "\n",
        "# Create a DataFrame to organize the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Sentiment Example': sentiment_texts,\n",
        "    'Tokens': token_lists,\n",
        "    'Filtered Tokens': filtered_token_lists,\n",
        "    'POS Tags': pos_tag_lists,\n",
        "    'Named Entities': ner_lists\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(results_df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K3E8rVsOkWHr"
      },
      "source": [
        "# Export Data to CSV - processed_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xyVoHpxui5Za"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Write the DataFrame to a CSV file named 'processed_data.csv' without including the index\n",
        "results_df.to_csv('processed_data.csv', index=False)\n",
        "\n",
        "# Read the CSV file 'processed_data.csv' into a Pandas DataFrame called processed_df\n",
        "# Specify the encoding as 'latin-1' to handle special characters if present\n",
        "processed_df = pd.read_csv('./processed_data.csv', encoding='latin-1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "3YzY2O4MkYeq",
        "outputId": "f7f4b7bf-f8e3-4524-b3f9-c2de3fe6d7cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment Example</th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Filtered Tokens</th>\n",
              "      <th>POS Tags</th>\n",
              "      <th>Named Entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"I love the new features of your product! It h...</td>\n",
              "      <td>['\"', 'I', 'love', 'the', 'new', 'features', '...</td>\n",
              "      <td>['\"', 'love', 'new', 'features', 'product', '!...</td>\n",
              "      <td>[('\"', 'PUNCT'), ('I', 'PRON'), ('love', 'VERB...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"The customer support was exceptional in New Y...</td>\n",
              "      <td>['\"', 'The', 'customer', 'support', 'was', 'ex...</td>\n",
              "      <td>['\"', 'customer', 'support', 'exceptional', 'N...</td>\n",
              "      <td>[('\"', 'PUNCT'), ('The', 'DET'), ('customer', ...</td>\n",
              "      <td>[('New York', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"The quality of your service exceeded my expec...</td>\n",
              "      <td>['\"', 'The', 'quality', 'of', 'your', 'service...</td>\n",
              "      <td>['\"', 'quality', 'service', 'exceeded', 'expec...</td>\n",
              "      <td>[('\"', 'PUNCT'), ('The', 'DET'), ('quality', '...</td>\n",
              "      <td>[('Prague', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"I'm extremely satisfied with my purchase. The...</td>\n",
              "      <td>['\"', 'I', \"'m\", 'extremely', 'satisfied', 'wi...</td>\n",
              "      <td>['\"', 'extremely', 'satisfied', 'purchase', '....</td>\n",
              "      <td>[('\"', 'PUNCT'), ('I', 'PRON'), (\"'m\", 'AUX'),...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"The user interface is intuitive and easy to n...</td>\n",
              "      <td>['\"', 'The', 'user', 'interface', 'is', 'intui...</td>\n",
              "      <td>['\"', 'user', 'interface', 'intuitive', 'easy'...</td>\n",
              "      <td>[('\"', 'PUNCT'), ('The', 'DET'), ('user', 'NOU...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Sentiment Example  \\\n",
              "0  \"I love the new features of your product! It h...   \n",
              "1  \"The customer support was exceptional in New Y...   \n",
              "2  \"The quality of your service exceeded my expec...   \n",
              "3  \"I'm extremely satisfied with my purchase. The...   \n",
              "4  \"The user interface is intuitive and easy to n...   \n",
              "\n",
              "                                              Tokens  \\\n",
              "0  ['\"', 'I', 'love', 'the', 'new', 'features', '...   \n",
              "1  ['\"', 'The', 'customer', 'support', 'was', 'ex...   \n",
              "2  ['\"', 'The', 'quality', 'of', 'your', 'service...   \n",
              "3  ['\"', 'I', \"'m\", 'extremely', 'satisfied', 'wi...   \n",
              "4  ['\"', 'The', 'user', 'interface', 'is', 'intui...   \n",
              "\n",
              "                                     Filtered Tokens  \\\n",
              "0  ['\"', 'love', 'new', 'features', 'product', '!...   \n",
              "1  ['\"', 'customer', 'support', 'exceptional', 'N...   \n",
              "2  ['\"', 'quality', 'service', 'exceeded', 'expec...   \n",
              "3  ['\"', 'extremely', 'satisfied', 'purchase', '....   \n",
              "4  ['\"', 'user', 'interface', 'intuitive', 'easy'...   \n",
              "\n",
              "                                            POS Tags         Named Entities  \n",
              "0  [('\"', 'PUNCT'), ('I', 'PRON'), ('love', 'VERB...                     []  \n",
              "1  [('\"', 'PUNCT'), ('The', 'DET'), ('customer', ...  [('New York', 'GPE')]  \n",
              "2  [('\"', 'PUNCT'), ('The', 'DET'), ('quality', '...    [('Prague', 'GPE')]  \n",
              "3  [('\"', 'PUNCT'), ('I', 'PRON'), (\"'m\", 'AUX'),...                     []  \n",
              "4  [('\"', 'PUNCT'), ('The', 'DET'), ('user', 'NOU...                     []  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
